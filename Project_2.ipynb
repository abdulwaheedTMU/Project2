{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulwaheedTMU/Project2/blob/main/Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Processing"
      ],
      "metadata": {
        "id": "rSuQSh7RuuqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define input image shape and batch size\n",
        "IMG_HEIGHT, IMG_WIDTH = 500, 500\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Set up directories\n",
        "train_dir = '/content/drive/MyDrive/Project 2 Data/Data/train'\n",
        "validation_dir = '/content/drive/MyDrive/Project 2 Data/Data/valid'\n",
        "test_dir = '/content/drive/MyDrive/Project 2 Data/Data/test'\n",
        "\n",
        "# Define augmentation pipeline for training data\n",
        "train_data_gen = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),  # Random flipping\n",
        "    tf.keras.layers.RandomRotation(0.2),  # Randomly rotate images\n",
        "    tf.keras.layers.RandomZoom(0.2),  # More aggressive zoom\n",
        "    tf.keras.layers.RandomBrightness(0.2)\n",
        "])\n",
        "\n",
        "# Rescaling only for validation and test data\n",
        "val_test_data_gen = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",  # Multi-class classification\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        ").map(lambda x, y: (train_data_gen(x, training=True), y))  # Apply augmentation pipeline\n",
        "\n",
        "# Create validation dataset\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        ").map(lambda x, y: (val_test_data_gen(x, training=False), y))  # Apply rescaling\n",
        "\n"
      ],
      "metadata": {
        "id": "6oAHr9V_jHJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Neural Network Architecture Design"
      ],
      "metadata": {
        "id": "bsLcH02EjHzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# 1. Convolutional Layer + Max Pooling Layer\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(500, 500, 3)))\n",
        "#model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 2. Add a second Convolutional Layer + Max Pooling Layer\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "#model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 3. Add a third Convolutional Layer + Max Pooling Layer\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "#model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "#model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 4. Flatten the output\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))  # First dense layer\n",
        "model.add(layers.Dropout(0.5))  # Dropout after first dense layer\n",
        "\n",
        "# model.add(layers.Dense(64, activation='relu'))  # Second dense layer\n",
        "# model.add(layers.Dropout(0.3))  # Dropout after second dense layer\n",
        "\n",
        "\n",
        "\n",
        "# 6. Output Layer\n",
        "model.add(layers.Dense(3, activation='softmax'))  # 3 neurons for 3 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "O1GzkptMjJ4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5bda8BHujcb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Hyperparameter Analysis"
      ],
      "metadata": {
        "id": "pyeabLUJjLfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "# Define model with tunable hyperparameters\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation=None, input_shape=(500, 500, 3)),  # No activation here\n",
        "    layers.LeakyReLU(alpha=0.1),  # LeakyReLU activation\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),  # Standard ReLU for comparison\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='elu'),  # ELU for dense layer\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(3, activation='softmax')  # Softmax for final layer\n",
        "])\n",
        "\n",
        "# Compile with a different optimizer (e.g., RMSprop)\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model without EarlyStopping\n",
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs= 10)\n",
        "\n"
      ],
      "metadata": {
        "id": "ticv1lKVjMAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Model Evaluation"
      ],
      "metadata": {
        "id": "ZAPez2MHjNi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-ownL0m0jN1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Model Testing"
      ],
      "metadata": {
        "id": "WWOTCEsQjQTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# List of test images with actual labels (modify paths and labels accordingly)\n",
        "test_images = [\n",
        "    {'path': '/content/drive/MyDrive/Project 2 Data/Data/test/crack/test_crack.jpg', 'label': 'Crack'},\n",
        "    {'path': '/content/drive/MyDrive/Project 2 Data/Data/test/missing-head/test_missinghead.jpg', 'label': 'Missing Head'},\n",
        "    {'path': '/content/drive/MyDrive/Project 2 Data/Data/test/paint-off/test_paintoff.jpg', 'label': 'Paint-Off'}\n",
        "]\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['crack', 'missing-head', 'paint-off']\n",
        "\n",
        "# Function to preprocess a single image\n",
        "def preprocess_image(img_path, target_size=(500, 500)):\n",
        "    img = load_img(img_path, target_size=target_size)\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array, np.array(img)  # Return both normalized array and original image as NumPy array\n",
        "\n",
        "# Predict, save, and display individual results\n",
        "for i, test_img in enumerate(test_images):\n",
        "    # Preprocess image\n",
        "    img_array, img = preprocess_image(test_img['path'])\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "    confidence = predictions[0][predicted_class] * 100\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    # Plot the image\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img.astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "    plt.title(\n",
        "        f\"Actual: {test_img['label']}\\n\"\n",
        "        f\"Predicted: {predicted_label}\\n\"\n",
        "        f\"Confidence: {confidence:.2f}%\"\n",
        "    )\n",
        "\n",
        "    # Save the image as a separate file\n",
        "    filename = f\"prediction_{i + 1}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    # Display the saved image\n",
        "    display(Image(filename=filename))\n",
        "\n"
      ],
      "metadata": {
        "id": "t0RzisPwjQkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}